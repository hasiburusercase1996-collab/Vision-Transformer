# -*- coding: utf-8 -*-
"""Coding ViT from Scartch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16xZwE7VdwfRM5hMRAC41Re_r05QXhXPq
"""

# import libaries
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
from torch.utils.data import dataloader

# transfromation for PIL to tensor format
transformation_operation = transforms.Compose([transforms.ToTensor()])

# import dataset
train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transformation_operation)
val_dataset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transformation_operation)

# define variable
num_classes = 10
batch_size = 64
num_channels  = 1  # as black and white image will be 3 for rgb
img_size = 28 # mnist data set is 28 by 28
patch_size = 7 # so 7by7
num_patches = (img_size // patch_size  )**2 # as 28/7 * 28 /7 ; // = floor operation to get integer value
embedding_dim  = 64
mlp_heads  = 128
attention_heads  = 4 # multihead attention
tranformer_blocks = 4
learning_rate  = .001
epochs = 5

# Define dataset batches
train_loader = dataloader.DataLoader(train_dataset, batch_size= batch_size, shuffle = True )
val_loader = dataloader.DataLoader(val_dataset, batch_size= batch_size, shuffle = True )

# Part 1: Patch embedding

class PatchEmbedding(nn.Module):
    def __init__(self):
      super().__init__()
      self.patch_embed = nn.Conv2d(num_channels,embedding_dim, kernel_size = patch_size , stride = patch_size)

    def forward(self,x):
      #patch embedding
      x = self.patch_embed(x) # convert the image of 1,28,28 to 64,4,4--> 64 the patchembed dims
      #flattening
      x= x.flatten(2)  # dim 2 means (4,4) whcih coverts (4,4) to 16
      x = x.transpose(1,2)
      return x
      # return self.patch_embed(x).flatten(2).transpose(1,2)
      # [class]
      #position enconding

data_point, label = next(iter(train_loader))

print('Shape of data point: ', data_point.shape)
print('Shape of label: ', label.shape)

patch_embed = nn.Conv2d(num_channels,embedding_dim, kernel_size = patch_size , stride = patch_size)
patch_embed_output = patch_embed(data_point)
print('Shape of patch embedding: ', patch_embed_output.shape)

patch_embed_output_flatten = patch_embed_output.flatten(2)
print('Shape of flattened patch embedding: ', patch_embed_output_flatten.shape)
print('Shape of flattened patch embedding: ', patch_embed_output_flatten.transpose(1,2).shape)

# can be done in one line using cascading methods

print(patch_embed(data_point).flatten(2).transpose(1,2).shape)

# Transformer encoder

class TransformerEncoder(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_norm1 = nn.LayerNorm(embedding_dim)
    self.layer_norm2 = nn.LayerNorm(embedding_dim)
    self.multihead_attention = nn.MultiheadAttention(embedding_dim,attention_heads, batch_first=True)
    self.mlp = nn.Sequential(
        nn.Linear(embedding_dim, mlp_heads),  # goes from 64 -->128 (hidden layer neurons)
        nn.GELU(),
        nn.Linear(mlp_heads,embedding_dim)   # comeback from 128 to 64 again
    )

  def forward(self,x):
    residual1  = x
    x = self.layer_norm1(x)
    x = self.multihead_attention(x,x,x)[0]  #x,x,x for Q,K, V
    x = x + residual1

    residual2 = x
    x = self.layer_norm2(x)
    x = self.mlp(x)
    x = x + residual2
    return x

# Part 3: MLP head classification(final MLP)

class MLP_head(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer_norm1 = nn.LayerNorm(embedding_dim)
    self.mlp_head = nn.Linear(embedding_dim, num_classes)

  def forward(self,x):
    x = self.layer_norm1(x)
    x = self.mlp_head(x)

    return x

class VisionTransformer(nn.Module):
  def __init__(self):
    super().__init__()
    self.patch_embedding = PatchEmbedding()

    # cls token
    self.cls_token = nn.Parameter(torch.randn(1,1,embedding_dim))  # 1 token, 1 channel and embed dim

    # position emnbedding
    self.position_embedding  = nn.Parameter(torch.randn(1,num_patches+1,embedding_dim )) #num_patches+1 for cls token +1

    # transformer blocks

    self.transformer_blocks  = nn.Sequential(
        *[TransformerEncoder() for _ in range(tranformer_blocks)]
        # manual option:  TransformerEncoder() ,TransformerEncoder() ,TransformerEncoder() ,TransformerEncoder()
    )


    # cls token pass into MLP head
    self.mlp_head  = MLP_head()

  def forward(self,x):

    x = self.patch_embedding(x)
    B = x.size(0)
    cls_token = self.cls_token.expand(B,-1,-1)
    x = torch.cat((cls_token,x), dim=1)
    x = x + self.position_embedding
    x = self.transformer_blocks(x)
    # we need to pass only cls_token to final mlp head only

    x = x [:,0]
    x = self.mlp_head(x)

    return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VisionTransformer().to(device)
optimzer = torch.optim.Adam(model.parameters(), lr = learning_rate)
criterion  = nn.CrossEntropyLoss()

# training

for epoch in range(epochs):
  model.train()
  train_loss = 0
  correct_epoch = 0
  total_epoch = 0
  print(f"\nEpoch {epoch+1}")

  for batch_idx, (images, labels) in enumerate(train_loader):
    images = images.to(device)
    labels = labels.to(device)
    optimzer.zero_grad()
    ouputs = model(images)
    loss = criterion(ouputs, labels)
    loss.backward()
    optimzer.step()

    train_loss += loss.item()
    preds  = ouputs.argmax(dim=1)
    correct  = (preds == labels).sum().item()
    accuracy = 100.0* correct / labels.size(0)

    correct_epoch += correct
    total_epoch += labels.size(0)

    if batch_idx % 100 == 0:
      print(f"Batch {batch_idx+1:3d}, Loss = {loss.item():.4f}, Accuracy = {accuracy:.2f}%")

  epoc_accuracy = 100.0 * correct_epoch / total_epoch
  print(f"Epoch ==> {epoch+1}, Summary: Total Loss ={train_loss:.4f}, Accuracy = {epoc_accuracy:.2f}%")

#

# model evalaution'
model.eval()
correct = 0
total  = 0

with torch.no_grad():
  for images, labels  in val_loader :
    images, labels = images.to(device), labels.to(device)
    outputs = model(images)
    preds = outputs.argmax(dim=1)
    correct += (preds == labels).sum().item()
    total += labels.size(0)

test_accuracy = 100.0 * correct / total
print(f"\n==> Val Accuracy = {test_accuracy:.2f}%")

import matplotlib.pyplot as plt

# show 10 predictions from the test batch

model.eval()
images, labels = next(iter(val_loader))
images = images.to(device)
labels = labels.to(device)

with torch.no_grad():
  outputs = model(images)
  preds = outputs.argmax(dim=1)

# move to cpu for plotting
images = images.cpu()
preds = preds.cpu()
labels = labels.cpu()

# plot first 10 images

plt.figure(figsize=(12,4))
for i in range(10):
  plt.subplot(2,5,i+1)
  plt.imshow(images[i][0], cmap='gray')
  plt.title(f"Pred: {preds[i].item()}, Label: {labels[i].item()}")
  plt.axis('off')

plt.show()